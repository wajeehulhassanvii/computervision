{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IOU\n",
    "IOU is an object detection evaluation metric based on the degree of overlap between the predicted bounding box and the ground truth bounding box (hand labeled). Let's have a look at the following derivative of IOU:\n",
    "\n",
    "\n",
    "The following image illustrates IOU, showing the predicted and ground truth bounding box of a large van:\n",
    "\n",
    "\n",
    "In this case, the IOU value is close to around 0.9 as the overlap area is very high. If the two bounding boxes do not overlap, then the IOU value is 0, and if they do overlap by 100%, then the IOU value is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does YOLO detect objects so fast?\n",
    "YOLO’s detection mechanism is based on a single Convolutional Neural Network (CNN) that simultaneously predicts multiple bounding boxes for objects and the probability of detection of a given object class in each bounding box. The following images illustrate this methodology:\n",
    "\n",
    "\n",
    "\n",
    "The preceding photos show the three major steps, from the development of bounding boxes to the use of non-max suppression and the final bounding boxes. The detailed steps to this are as follows:\n",
    "\n",
    "The CNN in YOLO uses features from the entire image to predict each bounding box. So, the prediction is global, rather than local.\n",
    "The entire image is divided into S x S grid cells and each grid cell predicts B bounding boxes and the probability (P) that the bounding box contains an object. So, there is a total of S x S x B bounding boxes, with corresponding probabilities for each bounding box.\n",
    "Each bounding box contains five predictions (x, y, w, h, and c), where the following applies:\n",
    "o (x, y) is the coordinate of the center of the bounding box, relative to the grid cell coordinate.\n",
    "o (w, h) is the width and height of the bounding box, relative to the image dimension.\n",
    "o (c) is the confidence prediction, representing IOU between the predicted box and ground truth box.\n",
    "The probability that a grid cell contains an object is defined as the probability of the class multiplied by the IOU value. This means that if a grid cell only partially contains an object, its probability will be low and the IOU value will remain low. It will have two effects on the bounding box from that grid cell:\n",
    "The bounding box shape will be smaller than the size of the bounding box from a grid cell that completely includes the object because the grid cell only sees part of the object and infers its shape from it. If the grid cell contains a very small part of an object, then it may not recognize the object at all.\n",
    "The bounding box class confidence level will be low because the IOU value resulting from the partial image will not fit the ground truth prediction.\n",
    "In general, each grid cell can contain only one class but using an anchor box principle, multiple classes can be assigned to a grid cell. An anchor box is a predefined shape that represents the shape of the classes being detected. For example, if we are detecting three classes—car, motorcycle, and human—then we can probably get by with two anchor box shapes—one representing the motorcycle and human, and the another representing the car. This can be confirmed by looking at the right-most image in the preceding images. We can determine the anchor box shape to form the training CSV data by analyzing the shape of each class using algorithms such as k-means clustering.\n",
    "Let's take the example of the preceding images. Here, we have three classes: car, motorcycle, and human. We are assuming a 5 x 5 grid with 2 anchor boxes and 8 dimensions (5 bounding box parameters (x, y, w, h, and c) and 3 classes (c1, c2, and c3)). So, the output vector size is 5 x 5 x 2 x 8. \n",
    "\n",
    "We repeat the Y = [x, y, w, h, c, c1, c2, c3, x, y, w, h, c, c1, c2, c3] parameter twice for each anchor box. The following images illustrates the calculation of the bounding box coordinates:\n",
    "\n",
    "\n",
    "\n",
    "The size of the image is 448 x 448. Here, for illustration purposes, the calculation methodology for two classes—human and car—is shown. Note that each anchor box size is 448/5 ~ 89."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('mytf2': conda)",
   "language": "python",
   "name": "python37764bitmytf2condad0e6e6c9b9bc4440a94687b08616bf38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
